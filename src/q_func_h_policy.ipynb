{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helpers import NormalizedEnv\n",
    "from helpers import RandomAgent\n",
    "import gym as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from helpers import NormalizedEnv\n",
    "\n",
    "from heuristicpolicy import HeuristicPendulumAgent\n",
    "from qnetwork import QNetwork\n",
    "from replaybuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating pendulum object\n",
    "pendulum = gym.make('Pendulum-v1', g=9.81)\n",
    "#Wrapping pendulum to map output space to [-1,1]\n",
    "pendulum = NormalizedEnv(pendulum)\n",
    "#Create random agent\n",
    "Random_Agent = RandomAgent(pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job will run on cpu\n"
     ]
    }
   ],
   "source": [
    "#set GPU for faster training\n",
    "cuda = torch.cuda.is_available() #check for CUDA\n",
    "device   = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Job will run on {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IT = 200\n",
    "BUFFER_SIZE = 1e4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPISODES = 1000\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(-1)\n",
    "\n",
    "heuristic_agent = HeuristicPendulumAgent(pendulum)\n",
    "network  = QNetwork().to(device) # critic\n",
    "memory = ReplayBuffer(max_size=BUFFER_SIZE)\n",
    "q_optimizer  = optim.Adam(network.parameters(),  lr=LEARNING_RATE)\n",
    "\n",
    "MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onestepTD(state_batch, action_batch, reward_batch, next_state_batch, trunc_batch, agent, gamma, network, iter):\n",
    "\n",
    "    state = torch.FloatTensor(state_batch).to(device)\n",
    "    action = torch.FloatTensor(action_batch).unsqueeze(1).to(device)\n",
    "    reward = torch.FloatTensor(np.array(reward_batch)).unsqueeze(1).to(device) #.unsqueeze(1)\n",
    "    trunc = torch.Tensor(np.float32(trunc_batch)).unsqueeze(1).to(device)\n",
    "    next_state= torch.FloatTensor(next_state_batch).to(device)\n",
    "    \n",
    "    action_next_state = agent.compute_action(state = next_state.T) \n",
    "\n",
    "    # To compute the targets in each step, do not use the on-trajectory action, \n",
    "    # but compute a new action according to the policy\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        network_input_state = torch.cat((state, action),1)\n",
    "        network_input_next_state = torch.cat((next_state, torch.Tensor(action_next_state).unsqueeze(1)),1)\n",
    "        q_next = network(network_input_next_state) # should not be differentiated \n",
    "\n",
    "    if iter == MAX_IT:\n",
    "        q_next = 0\n",
    "    target_state = reward + gamma * q_next \n",
    "\n",
    "    q = network(network_input_state)\n",
    "\n",
    "    q_optimizer.zero_grad()\n",
    "    q_loss = MSE(target_state, q)\n",
    "    q_loss.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    return q_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward = []\n",
    "plot_policy = []\n",
    "plot_q = []\n",
    "plot_steps = []\n",
    "\n",
    "\n",
    "best_reward = -np.inf\n",
    "saved_reward = -np.inf\n",
    "saved_ep = 0\n",
    "average_reward = 0\n",
    "global_step = 0\n",
    "nr_of_samples = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<24:35,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.740352630615234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [06:48<1:17:26,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.805213928222656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [13:20<48:23,  3.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.628854751586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [19:44<52:55,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.099409103393555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [26:31<40:09,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.112615585327148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [33:16<32:19,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.060672760009766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [40:15<29:45,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.915605545043945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [46:56<18:54,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.09168529510498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 798/1000 [53:17<13:20,  3.96s/it]"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(NUM_EPISODES)):\n",
    "    current_state = deepcopy(pendulum.reset()[0])\n",
    "\n",
    "    ep_reward = 0.\n",
    "    ep_q_value = 0.\n",
    "    step = 0\n",
    "\n",
    "    # collect experience\n",
    "    for i in range(MAX_IT):\n",
    "        # how many iterations??\n",
    "        action = heuristic_agent.compute_action(state = current_state)\n",
    "        transformed_action = pendulum.action(action)\n",
    "        next_state, reward, term, trunc, info = pendulum.step(transformed_action)\n",
    "\n",
    "        memory.add_transition(state = current_state, action = action, reward = reward, next_state = next_state, trunc = trunc)\n",
    "\n",
    "        if memory.count() > nr_of_samples:\n",
    "            # sample a batch of transitions from the replay buffer\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, trunc_batch = memory.sample_transition(nr_of_samples)\n",
    "\n",
    "            # 1-step TD-learning rule\n",
    "            q_loss = onestepTD(state_batch, action_batch, reward_batch, next_state_batch, trunc_batch, heuristic_agent, GAMMA, network, i)\n",
    "                       \n",
    "        \n",
    "        current_state = deepcopy(next_state)\n",
    "        ep_reward += reward\n",
    "    try:\n",
    "        plot_reward.append([ep_reward, episode+1])\n",
    "        plot_q.append([q_loss.data, episode+1])\n",
    "\n",
    "        if (episode % 100 == 0):\n",
    "            print(q_loss.item())\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = list(zip(*plot_q))\n",
    "plt.plot(list(q[1]), list(q[0]), 'g') #row=0, col=1\n",
    "plt.title('The Mean Squared error of the Q value of the critic network')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
